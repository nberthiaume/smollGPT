{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import math\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "block_size = 1024\n",
    "save_path  = f'./dataset/wikipedia_ctx_{block_size}.dat'\n",
    "num_proc   = cpu_count() # Number of processes to use when working with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"p50k_base\")\n",
    "assert tokenizer.decode(tokenizer.encode(\"dogs are cool\")) == \"dogs are cool\"\n",
    "vocab_size = tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from HF\n",
    "\n",
    "Using the `lucadiliello/english_wikipedia` dataset. This is about 6GB of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_dataset = load_dataset(\"lucadiliello/english_wikipedia\", split='train') # by default, the HF dataset puts everything into train.\n",
    "wikipedia_dataset = wikipedia_dataset.remove_columns(['filename', 'source_domain', 'title', 'url'])\n",
    "# wikipedia_dataset = wikipedia_dataset.select(range(12000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Epeli Kanakana\\n\\nRatu Epeli Kanakana (died 2010) was a Fijian chief. He held the title of Tui Suva, and was the traditional ruler of the area that includes the city of Suva, the nation's capital. The title of Tui Suva is only kept within the Naivutuvutu family of the Tokatoka Solia of Mataqali Vuanimocelolo of the Yavusa Vatuwaqa.\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output something random - making sure it works properly\n",
    "wikipedia_dataset[int(torch.randint(0, len(wikipedia_dataset) - 1, (1,)))]['maintext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Hugging Face dataset into a bunch of tensors\n",
    "\n",
    "The following steps will transform all these english articles into chunks of tokens of length `block_size`.\n",
    "\n",
    "Doing this upfront will speedup the training process as we won't need to tokenize and move each batch to the CPU as we go through our dataset. \n",
    "\n",
    "This will allow us to only work with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=12): 100%|██████████| 4184712/4184712 [03:57<00:00, 17590.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dataset: 2.28B\n"
     ]
    }
   ],
   "source": [
    "eot_token = tokenizer.eot_token\n",
    "\n",
    "def tokenize(x):\n",
    "    tokens = tokenizer.encode(x['maintext'])\n",
    "    tokens.append(eot_token)\n",
    "    return { 'tokens':  tokens, 'n_tokens': len(tokens) }\n",
    "\n",
    "tokenized_dataset = wikipedia_dataset.map(tokenize, num_proc=num_proc, remove_columns=[\"maintext\"])\n",
    "del wikipedia_dataset\n",
    "total_tokens = sum(tokenized_dataset['n_tokens'])\n",
    "print(f\"Number of tokens in dataset: {total_tokens / 1e9 :.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dataset(dataset, n_chunks=12):\n",
    "    n_rows        = len(dataset)\n",
    "    chunk_size    = math.floor(n_rows / (n_chunks - 1))\n",
    "    segments = [ dataset.select(range( i*chunk_size, (i+1)*chunk_size )) for i in range(n_chunks - 1) ]\n",
    "\n",
    "    last_segment = dataset.select(range( (n_chunks - 1) * chunk_size, n_rows - 1 ))\n",
    "    segments.append(last_segment)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(dataset_chunk):\n",
    "    n_tokens = sum(dataset_chunk['n_tokens'])\n",
    "    arr = np.empty(n_tokens, dtype=np.uint16)\n",
    "    ptr = 0\n",
    "    for v in dataset_chunk:\n",
    "        tokens = v['tokens']\n",
    "        arr[ptr:ptr+len(tokens)] = tokens\n",
    "        ptr += len(tokens)\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == Processing chunk 0 ==\n",
      "==== Chunk 0 took: 116.30s =====\n",
      " == Processing chunk 1 ==\n",
      "==== Chunk 1 took: 59.70s =====\n",
      " == Processing chunk 2 ==\n",
      "==== Chunk 2 took: 45.88s =====\n",
      " == Processing chunk 3 ==\n",
      "==== Chunk 3 took: 39.12s =====\n",
      " == Processing chunk 4 ==\n",
      "==== Chunk 4 took: 38.96s =====\n",
      " == Processing chunk 5 ==\n",
      "==== Chunk 5 took: 37.12s =====\n",
      " == Processing chunk 6 ==\n",
      "==== Chunk 6 took: 34.74s =====\n",
      " == Processing chunk 7 ==\n",
      "==== Chunk 7 took: 34.62s =====\n",
      " == Processing chunk 8 ==\n",
      "==== Chunk 8 took: 32.06s =====\n",
      " == Processing chunk 9 ==\n",
      "==== Chunk 9 took: 0.00s =====\n"
     ]
    }
   ],
   "source": [
    "segments = chunk_dataset(tokenized_dataset, n_chunks=10)\n",
    "all_tokens = np.empty(total_tokens, dtype=np.uint16)\n",
    "\n",
    "ptr = 0\n",
    "for i, chunk in enumerate(segments):\n",
    "    print(f\" == Processing chunk {i} ==\")\n",
    "    t0 = time.time()\n",
    "    tokens = process_chunk(chunk)\n",
    "    all_tokens[ptr:ptr+len(tokens)] = tokens\n",
    "    ptr += len(tokens)\n",
    "\n",
    "    del tokens\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"==== Chunk {i} took: {t1 - t0 :.2f}s =====\")\n",
    "\n",
    "\n",
    "chunk_size = block_size + 1\n",
    "\n",
    "n_excess_tokens = len(all_tokens) % chunk_size \n",
    "all_tokens = all_tokens[:len(all_tokens) - n_excess_tokens]\n",
    "\n",
    "chunks_in_dataset = len(all_tokens) // chunk_size\n",
    "new_shape = (chunks_in_dataset, chunk_size)\n",
    "\n",
    "all_tokens = all_tokens.reshape(new_shape)\n",
    "\n",
    "memmap = np.memmap(save_path, dtype=np.uint16, mode='w+', shape=all_tokens.shape)\n",
    "memmap[:] = all_tokens\n",
    "del memmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smollGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
